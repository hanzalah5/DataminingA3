{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test.csv\")\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test_label = pd.read_csv(\"data/test_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132481, 26)\n",
      "(87841, 26)\n",
      "(87841, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_(min)</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132481.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132482.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132483.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132484.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>132485.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>132486.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>132487.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>132488.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>132489.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>132490.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>132491.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>132492.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>132493.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>132494.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>132495.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>132496.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>132497.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>132498.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>132499.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp_(min)  label\n",
       "0          132480.0      0\n",
       "1          132481.0      0\n",
       "2          132482.0      0\n",
       "3          132483.0      0\n",
       "4          132484.0      0\n",
       "5          132485.0      0\n",
       "6          132486.0      0\n",
       "7          132487.0      0\n",
       "8          132488.0      0\n",
       "9          132489.0      0\n",
       "10         132490.0      0\n",
       "11         132491.0      0\n",
       "12         132492.0      0\n",
       "13         132493.0      0\n",
       "14         132494.0      0\n",
       "15         132495.0      0\n",
       "16         132496.0      0\n",
       "17         132497.0      0\n",
       "18         132498.0      0\n",
       "19         132499.0      0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_(min)</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132480.0</td>\n",
       "      <td>0.775374</td>\n",
       "      <td>0.909185</td>\n",
       "      <td>0.606704</td>\n",
       "      <td>0.660626</td>\n",
       "      <td>0.449968</td>\n",
       "      <td>0.426717</td>\n",
       "      <td>0.471591</td>\n",
       "      <td>0.434668</td>\n",
       "      <td>0.479511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400617</td>\n",
       "      <td>0.480444</td>\n",
       "      <td>0.588670</td>\n",
       "      <td>0.404036</td>\n",
       "      <td>0.638957</td>\n",
       "      <td>0.020236</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>0.173375</td>\n",
       "      <td>0.008715</td>\n",
       "      <td>0.105991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132481.0</td>\n",
       "      <td>0.775423</td>\n",
       "      <td>0.909142</td>\n",
       "      <td>0.607796</td>\n",
       "      <td>0.660655</td>\n",
       "      <td>0.487813</td>\n",
       "      <td>0.442545</td>\n",
       "      <td>0.505682</td>\n",
       "      <td>0.448640</td>\n",
       "      <td>0.499451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396902</td>\n",
       "      <td>0.500213</td>\n",
       "      <td>0.587041</td>\n",
       "      <td>0.421605</td>\n",
       "      <td>0.637865</td>\n",
       "      <td>0.018550</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>0.191826</td>\n",
       "      <td>0.010893</td>\n",
       "      <td>0.110599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132482.0</td>\n",
       "      <td>0.775458</td>\n",
       "      <td>0.909004</td>\n",
       "      <td>0.607988</td>\n",
       "      <td>0.660623</td>\n",
       "      <td>0.469532</td>\n",
       "      <td>0.433682</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.450906</td>\n",
       "      <td>0.487223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389270</td>\n",
       "      <td>0.487982</td>\n",
       "      <td>0.582404</td>\n",
       "      <td>0.407393</td>\n",
       "      <td>0.636091</td>\n",
       "      <td>0.018550</td>\n",
       "      <td>0.039146</td>\n",
       "      <td>0.188361</td>\n",
       "      <td>0.010893</td>\n",
       "      <td>0.115207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132483.0</td>\n",
       "      <td>0.775513</td>\n",
       "      <td>0.909200</td>\n",
       "      <td>0.607218</td>\n",
       "      <td>0.660640</td>\n",
       "      <td>0.459910</td>\n",
       "      <td>0.435581</td>\n",
       "      <td>0.482955</td>\n",
       "      <td>0.454683</td>\n",
       "      <td>0.490431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403780</td>\n",
       "      <td>0.491253</td>\n",
       "      <td>0.584660</td>\n",
       "      <td>0.407141</td>\n",
       "      <td>0.638138</td>\n",
       "      <td>0.020236</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>0.186975</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.110599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132484.0</td>\n",
       "      <td>0.775587</td>\n",
       "      <td>0.909318</td>\n",
       "      <td>0.606654</td>\n",
       "      <td>0.660643</td>\n",
       "      <td>0.462155</td>\n",
       "      <td>0.436214</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.449396</td>\n",
       "      <td>0.494671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403636</td>\n",
       "      <td>0.495947</td>\n",
       "      <td>0.581527</td>\n",
       "      <td>0.402761</td>\n",
       "      <td>0.632678</td>\n",
       "      <td>0.021922</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>0.156325</td>\n",
       "      <td>0.008715</td>\n",
       "      <td>0.110599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>132485.0</td>\n",
       "      <td>0.775697</td>\n",
       "      <td>0.909187</td>\n",
       "      <td>0.606464</td>\n",
       "      <td>0.660658</td>\n",
       "      <td>0.471456</td>\n",
       "      <td>0.450459</td>\n",
       "      <td>0.471591</td>\n",
       "      <td>0.473943</td>\n",
       "      <td>0.496312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416948</td>\n",
       "      <td>0.497227</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.390703</td>\n",
       "      <td>0.632542</td>\n",
       "      <td>0.020236</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>0.143505</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.115207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>132486.0</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.909128</td>\n",
       "      <td>0.606628</td>\n",
       "      <td>0.660697</td>\n",
       "      <td>0.482040</td>\n",
       "      <td>0.461855</td>\n",
       "      <td>0.482955</td>\n",
       "      <td>0.482628</td>\n",
       "      <td>0.493692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424589</td>\n",
       "      <td>0.495093</td>\n",
       "      <td>0.602832</td>\n",
       "      <td>0.400303</td>\n",
       "      <td>0.635818</td>\n",
       "      <td>0.023609</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>0.156094</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.133641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>132487.0</td>\n",
       "      <td>0.775863</td>\n",
       "      <td>0.909189</td>\n",
       "      <td>0.606640</td>\n",
       "      <td>0.660709</td>\n",
       "      <td>0.470494</td>\n",
       "      <td>0.451725</td>\n",
       "      <td>0.482955</td>\n",
       "      <td>0.468656</td>\n",
       "      <td>0.496300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423860</td>\n",
       "      <td>0.498080</td>\n",
       "      <td>0.607344</td>\n",
       "      <td>0.392137</td>\n",
       "      <td>0.629948</td>\n",
       "      <td>0.023609</td>\n",
       "      <td>0.039146</td>\n",
       "      <td>0.169102</td>\n",
       "      <td>0.010893</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>132488.0</td>\n",
       "      <td>0.775781</td>\n",
       "      <td>0.909135</td>\n",
       "      <td>0.606152</td>\n",
       "      <td>0.660724</td>\n",
       "      <td>0.474984</td>\n",
       "      <td>0.471352</td>\n",
       "      <td>0.505682</td>\n",
       "      <td>0.480363</td>\n",
       "      <td>0.505251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446689</td>\n",
       "      <td>0.508036</td>\n",
       "      <td>0.602958</td>\n",
       "      <td>0.400112</td>\n",
       "      <td>0.634180</td>\n",
       "      <td>0.023609</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>0.154737</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.129032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>132489.0</td>\n",
       "      <td>0.775720</td>\n",
       "      <td>0.908909</td>\n",
       "      <td>0.606276</td>\n",
       "      <td>0.660735</td>\n",
       "      <td>0.474343</td>\n",
       "      <td>0.462172</td>\n",
       "      <td>0.494318</td>\n",
       "      <td>0.479607</td>\n",
       "      <td>0.499383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410279</td>\n",
       "      <td>0.501209</td>\n",
       "      <td>0.597945</td>\n",
       "      <td>0.404828</td>\n",
       "      <td>0.635408</td>\n",
       "      <td>0.020236</td>\n",
       "      <td>0.039146</td>\n",
       "      <td>0.148890</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.138249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp_(min)  feature_0  feature_1  feature_2  feature_3  feature_4  \\\n",
       "0         132480.0   0.775374   0.909185   0.606704   0.660626   0.449968   \n",
       "1         132481.0   0.775423   0.909142   0.607796   0.660655   0.487813   \n",
       "2         132482.0   0.775458   0.909004   0.607988   0.660623   0.469532   \n",
       "3         132483.0   0.775513   0.909200   0.607218   0.660640   0.459910   \n",
       "4         132484.0   0.775587   0.909318   0.606654   0.660643   0.462155   \n",
       "5         132485.0   0.775697   0.909187   0.606464   0.660658   0.471456   \n",
       "6         132486.0   0.775862   0.909128   0.606628   0.660697   0.482040   \n",
       "7         132487.0   0.775863   0.909189   0.606640   0.660709   0.470494   \n",
       "8         132488.0   0.775781   0.909135   0.606152   0.660724   0.474984   \n",
       "9         132489.0   0.775720   0.908909   0.606276   0.660735   0.474343   \n",
       "\n",
       "   feature_5  feature_6  feature_7  feature_8  ...  feature_15  feature_16  \\\n",
       "0   0.426717   0.471591   0.434668   0.479511  ...    0.400617    0.480444   \n",
       "1   0.442545   0.505682   0.448640   0.499451  ...    0.396902    0.500213   \n",
       "2   0.433682   0.477273   0.450906   0.487223  ...    0.389270    0.487982   \n",
       "3   0.435581   0.482955   0.454683   0.490431  ...    0.403780    0.491253   \n",
       "4   0.436214   0.477273   0.449396   0.494671  ...    0.403636    0.495947   \n",
       "5   0.450459   0.471591   0.473943   0.496312  ...    0.416948    0.497227   \n",
       "6   0.461855   0.482955   0.482628   0.493692  ...    0.424589    0.495093   \n",
       "7   0.451725   0.482955   0.468656   0.496300  ...    0.423860    0.498080   \n",
       "8   0.471352   0.505682   0.480363   0.505251  ...    0.446689    0.508036   \n",
       "9   0.462172   0.494318   0.479607   0.499383  ...    0.410279    0.501209   \n",
       "\n",
       "   feature_17  feature_18  feature_19  feature_20  feature_21  feature_22  \\\n",
       "0    0.588670    0.404036    0.638957    0.020236    0.042705    0.173375   \n",
       "1    0.587041    0.421605    0.637865    0.018550    0.042705    0.191826   \n",
       "2    0.582404    0.407393    0.636091    0.018550    0.039146    0.188361   \n",
       "3    0.584660    0.407141    0.638138    0.020236    0.042705    0.186975   \n",
       "4    0.581527    0.402761    0.632678    0.021922    0.042705    0.156325   \n",
       "5    0.582278    0.390703    0.632542    0.020236    0.042705    0.143505   \n",
       "6    0.602832    0.400303    0.635818    0.023609    0.042705    0.156094   \n",
       "7    0.607344    0.392137    0.629948    0.023609    0.039146    0.169102   \n",
       "8    0.602958    0.400112    0.634180    0.023609    0.042705    0.154737   \n",
       "9    0.597945    0.404828    0.635408    0.020236    0.039146    0.148890   \n",
       "\n",
       "   feature_23  feature_24  \n",
       "0    0.008715    0.105991  \n",
       "1    0.010893    0.110599  \n",
       "2    0.010893    0.115207  \n",
       "3    0.013072    0.110599  \n",
       "4    0.008715    0.110599  \n",
       "5    0.013072    0.115207  \n",
       "6    0.013072    0.133641  \n",
       "7    0.010893    0.142857  \n",
       "8    0.019608    0.129032  \n",
       "9    0.006536    0.138249  \n",
       "\n",
       "[10 rows x 26 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp_(min)    0\n",
       "feature_0          0\n",
       "feature_1          0\n",
       "feature_2          0\n",
       "feature_3          0\n",
       "feature_4          0\n",
       "feature_5          0\n",
       "feature_6          0\n",
       "feature_7          0\n",
       "feature_8          0\n",
       "feature_9          0\n",
       "feature_10         0\n",
       "feature_11         0\n",
       "feature_12         0\n",
       "feature_13         0\n",
       "feature_14         0\n",
       "feature_15         0\n",
       "feature_16         0\n",
       "feature_17         0\n",
       "feature_18         0\n",
       "feature_19         0\n",
       "feature_20         0\n",
       "feature_21         0\n",
       "feature_22         0\n",
       "feature_23         0\n",
       "feature_24         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tensor = tf.convert_to_tensor(test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels from the dataframe\n",
    "test_labels = test_label['label'].to_numpy()\n",
    "\n",
    "# Create a TensorFlow dataset from the labels\n",
    "test_label_tensor = tf.convert_to_tensor(test_labels, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([87841, 26])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_tensor.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp_(min)       0\n",
       "feature_0             0\n",
       "feature_1             0\n",
       "feature_2             0\n",
       "feature_3             0\n",
       "feature_4           202\n",
       "feature_5           249\n",
       "feature_6           563\n",
       "feature_7            47\n",
       "feature_8            47\n",
       "feature_9             0\n",
       "feature_10           47\n",
       "feature_11            0\n",
       "feature_12            0\n",
       "feature_13          249\n",
       "feature_14          202\n",
       "feature_15            0\n",
       "feature_16            0\n",
       "feature_17            0\n",
       "feature_18          202\n",
       "feature_19            0\n",
       "feature_20            0\n",
       "feature_21         2136\n",
       "feature_22          204\n",
       "feature_23            0\n",
       "feature_24           47\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filled = train.fillna(train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp_(min)    0\n",
       "feature_0          0\n",
       "feature_1          0\n",
       "feature_2          0\n",
       "feature_3          0\n",
       "feature_4          0\n",
       "feature_5          0\n",
       "feature_6          0\n",
       "feature_7          0\n",
       "feature_8          0\n",
       "feature_9          0\n",
       "feature_10         0\n",
       "feature_11         0\n",
       "feature_12         0\n",
       "feature_13         0\n",
       "feature_14         0\n",
       "feature_15         0\n",
       "feature_16         0\n",
       "feature_17         0\n",
       "feature_18         0\n",
       "feature_19         0\n",
       "feature_20         0\n",
       "feature_21         0\n",
       "feature_22         0\n",
       "feature_23         0\n",
       "feature_24         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filled.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filled_values = train_filled.values\n",
    "\n",
    "# Function to generate geometric distribution masks\n",
    "def generate_masks(data, mask_length=5, mask_width=0.1, p=0.5):\n",
    "    augmented_data = []\n",
    "    for sequence in data:\n",
    "        augmented_sequence = sequence.copy()\n",
    "        for i in range(len(sequence)):\n",
    "            if np.random.uniform() < p:\n",
    "                mask_start = np.random.randint(0, len(sequence) - mask_length)\n",
    "                mask_end = min(len(sequence), mask_start + mask_length)\n",
    "                mask = np.random.normal(1, mask_width, mask_end - mask_start)\n",
    "                augmented_sequence[mask_start:mask_end] *= mask\n",
    "        augmented_data.append(augmented_sequence)\n",
    "    return np.array(augmented_data)\n",
    "\n",
    "# Assuming your training data is stored in a numpy array named 'train_data'\n",
    "# Apply data augmentation\n",
    "augmented_train_data = generate_masks(train_filled_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132481, 26)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming 'train_data' and 'val_data' are numpy arrays\n",
    "train_data, val_data = train_test_split(augmented_train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "train_mean = train_data.mean(axis=0)\n",
    "train_std = train_data.std(axis=0)\n",
    "train_data = (train_data - train_mean) / train_std\n",
    "val_data = (val_data - train_mean) / train_std\n",
    "\n",
    "# Assuming you have separate datasets for normal and abnormal samples\n",
    "normal_data, abnormal_data = train_test_split(train_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Assuming you have a train_data_loader that iterates over your training data in batches\n",
    "normal_dataset = tf.data.Dataset.from_tensor_slices(normal_data)\n",
    "normal_dataset = normal_dataset.shuffle(buffer_size=len(normal_data)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "abnormal_dataset = tf.data.Dataset.from_tensor_slices(abnormal_data)\n",
    "abnormal_dataset = abnormal_dataset.shuffle(buffer_size=len(abnormal_data)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Combine normal and abnormal datasets into pairs\n",
    "combined_dataset = tf.data.Dataset.zip((normal_dataset, abnormal_dataset))\n",
    "combined_dataset = combined_dataset.shuffle(buffer_size=len(normal_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "train_data, val_data = train_test_split(augmented_train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "train_mean = train_data.mean(axis=0)\n",
    "train_std = train_data.std(axis=0)\n",
    "train_data = (train_data - train_mean) / train_std\n",
    "val_data = (val_data - train_mean) / train_std\n",
    "\n",
    "\n",
    "# Assuming 'train_data' and 'val_data' are numpy arrays\n",
    "train_data_tensor = tf.convert_to_tensor(train_data, dtype=tf.float32)\n",
    "val_data_tensor = tf.convert_to_tensor(val_data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape: (26,)\n",
      "x shape in encoder: (None, 1, 26)\n",
      "x shape in encoder: (None, 1, 512)\n",
      "x shape in decoder: (None, 1, 512)\n",
      "x shape in decoder: (None, 1, 512)\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_5 (TFOpLambda)  (None, 1, 26)        0           ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " multi_head_attention_12 (Multi  (None, 1, 26)       11154       ['tf.expand_dims_5[0][0]',       \n",
      " HeadAttention)                                                   'tf.expand_dims_5[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 1, 26)        0           ['multi_head_attention_12[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_24 (LayerN  (None, 1, 26)       52          ['dropout_24[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 1, 512)       13824       ['layer_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 1, 512)       0           ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_25 (LayerN  (None, 1, 512)      1024        ['dropout_25[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_13 (Multi  (None, 1, 512)      213816      ['layer_normalization_25[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 1, 512)       0           ['multi_head_attention_13[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_26 (LayerN  (None, 1, 512)      1024        ['dropout_26[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 1, 512)       262656      ['layer_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 1, 512)       0           ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_27 (LayerN  (None, 1, 512)      1024        ['dropout_27[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (Multi  (None, 1, 512)      213816      ['layer_normalization_27[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 1, 512)       0           ['multi_head_attention_14[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_28 (LayerN  (None, 1, 512)      1024        ['dropout_28[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 1, 512)       262656      ['layer_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)           (None, 1, 512)       0           ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_29 (LayerN  (None, 1, 512)      1024        ['dropout_29[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_15 (Multi  (None, 1, 512)      213816      ['layer_normalization_29[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 1, 512)       0           ['multi_head_attention_15[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_30 (LayerN  (None, 1, 512)      1024        ['dropout_30[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 1, 512)       262656      ['layer_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 1, 512)       0           ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_31 (LayerN  (None, 1, 512)      1024        ['dropout_31[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 1, 26)        13338       ['layer_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,474,952\n",
      "Trainable params: 1,474,952\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def transformer_autoencoder(input_shape, num_heads=4, ff_dim=512, num_layers=2):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = tf.expand_dims(inputs, 1)  # Add an extra dimension\n",
    "    d_model = input_shape[-1]  # Set d_model to the last dimension of the input_shape\n",
    "    for _ in range(num_layers):\n",
    "        print(\"x shape in encoder:\", x.shape)\n",
    "        x = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    # Decoder\n",
    "    for _ in range(num_layers):\n",
    "        print(\"x shape in decoder:\", x.shape)\n",
    "        x = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(input_shape[-1])(x)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Assuming your input shape is the shape of your training data tensors\n",
    "input_shape = train_data_tensor.shape[1:]\n",
    "print(\"input_shape:\", input_shape)\n",
    "\n",
    "# Create the model\n",
    "model = transformer_autoencoder(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the encoder model\n",
    "def encoder_model(input_shape, latent_dim=64):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Reshape input to include channel dimension\n",
    "    x = tf.expand_dims(inputs, axis=-1)\n",
    "    \n",
    "    # Add convolutional layers\n",
    "    x = layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # Add more convolutional layers if needed\n",
    "    x = layers.Conv1D(128, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # Flatten the output and feed into a dense layer\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(latent_dim, activation='relu')(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Assuming your input shape is (26,) and you want a latent dimension of 64\n",
    "input_shape = (26,)\n",
    "encoder = encoder_model(input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming 'train_data' and 'val_data' are numpy arrays\n",
    "train_data_tensor = tf.convert_to_tensor(train_data, dtype=tf.float32)\n",
    "val_data_tensor = tf.convert_to_tensor(val_data, dtype=tf.float32)\n",
    "\n",
    "# Define your contrastive loss function\n",
    "class ContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def call(self, emb1, emb2):\n",
    "        batch_size = tf.shape(emb1)[0]\n",
    "        labels = tf.concat([tf.ones((batch_size,)), tf.zeros((batch_size,))], axis=0)\n",
    "        distances = tf.norm(emb1 - emb2, axis=-1)\n",
    "        positive_loss = labels[:batch_size] * tf.square(distances)\n",
    "        negative_loss = labels[batch_size:] * tf.square(tf.maximum(self.margin - distances, 0))\n",
    "        return tf.reduce_mean(positive_loss + negative_loss)\n",
    "\n",
    "\n",
    "# Set up your optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Instantiate your contrastive loss\n",
    "contrastive_loss = ContrastiveLoss()\n",
    "\n",
    "# Define the training loop\n",
    "@tf.function\n",
    "def train_step(model, data, optimizer, contrastive_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        embeddings = model(data)\n",
    "        loss = contrastive_loss(embeddings[:len(train_data)], embeddings[len(train_data):])\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Assuming you iterate over training data directly in batches\n",
    "num_epochs = 10\n",
    "num_batches = len(train_data) // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        batch_data = train_data_tensor[start:end]\n",
    "        # Forward pass and update weights\n",
    "        loss = train_step(encoder, batch_data, optimizer, contrastive_loss)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.numpy():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the Generator\n",
    "def build_generator(latent_dim, output_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_dim=latent_dim))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(output_shape, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def build_discriminator(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(256, activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Define the GAN\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = models.Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "# Compile the Discriminator\n",
    "discriminator = build_discriminator(input_shape)\n",
    "discriminator.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Compile the GAN\n",
    "latent_dim = 100  # Dimensionality of the latent space\n",
    "output_shape = len(normal_samples[0])  # Output shape of the generator\n",
    "generator = build_generator(latent_dim, output_shape)\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for normal_samples, _ in train_dataset:\n",
    "        # Generate random noise\n",
    "        noise = tf.random.normal((batch_size, latent_dim))\n",
    "        \n",
    "        # Generate synthetic normal data using the generator\n",
    "        generated_samples = generator.predict(noise)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        real_labels = tf.ones((batch_size, 1))\n",
    "        fake_labels = tf.zeros((batch_size, 1))\n",
    "        discriminator.trainable = True\n",
    "        d_loss_real = discriminator.train_on_batch(normal_samples, real_labels)\n",
    "        d_loss_fake = discriminator.train_on_batch(generated_samples, fake_labels)\n",
    "        d_loss = 0.5 * tf.math.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # Train the generator (via the GAN model)\n",
    "        noise = tf.random.normal((batch_size, latent_dim))\n",
    "        valid_labels = tf.ones((batch_size, 1))\n",
    "        discriminator.trainable = False\n",
    "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
    "        \n",
    "        # Print training progress\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Batch {batch + 1}/{len(train_dataset)}, D_loss={d_loss}, G_loss={g_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.27759562841530055\n",
      "Recall: 0.031253845207333576\n",
      "F1-Score: 0.0561822605618226\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "predicted_labels = np.zeros(len(test_data_tensor))\n",
    "anomalies_list = [anomaly.numpy() for anomaly in anomalies]\n",
    "anomaly_indices = [i for i, sample in enumerate(test_data_tensor) if any(np.array_equal(sample.numpy(), anomaly) for anomaly in anomalies_list)]\n",
    "\n",
    "\n",
    "predicted_labels[anomaly_indices] = 1\n",
    "true_labels = np.array(test_label['label'])\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
